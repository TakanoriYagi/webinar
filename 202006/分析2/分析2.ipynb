{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "これはテストです。\n",
      "[0, 'これ', '此れ', 'PRON', '代名詞']\n",
      "[1, 'は', 'は', 'ADP', '助詞-係助詞']\n",
      "[2, 'テスト', 'テスト', 'NOUN', '名詞-普通名詞-サ変可能']\n",
      "[3, 'です', 'です', 'AUX', '助動詞']\n",
      "[4, '。', '。', 'PUNCT', '補助記号-句点']\n",
      "6月のセミナーです。\n",
      "[5, '6', '6', 'NUM', '名詞-数詞']\n",
      "[6, '月', '月', 'NOUN', '名詞-普通名詞-助数詞可能']\n",
      "[7, 'の', 'の', 'ADP', '助詞-格助詞']\n",
      "[8, 'セミナー', 'セミナー', 'NOUN', '名詞-普通名詞-一般']\n",
      "[9, 'です', 'です', 'AUX', '助動詞']\n",
      "[10, '。', '。', 'PUNCT', '補助記号-句点']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('ja_ginza')\n",
    "doc = nlp('これはテストです。6月のセミナーです。')\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    for token in sent:\n",
    "        info = [\n",
    "            token.i,         # トークン番号\n",
    "            token.orth_,     # テキスト\n",
    "            token.lemma_,    # 基本形\n",
    "            token.pos_,      # 品詞\n",
    "            token.tag_,      # 品詞詳細\n",
    "        ]\n",
    "        print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('ja_ginza')\n",
    "\n",
    "with open('bocchan.txt', 'r',encoding=\"utf-8\", errors=\"ignore\") as f1, open('bocchan2.txt', 'w', encoding=\"utf-8\", errors=\"ignore\") as f2:\n",
    "    for line in f1:\n",
    "        for sent in nlp(line).sents:\n",
    "            for token in sent:\n",
    "                f2.write(f'{token.i}\\t{token.orth_}\\t{token.lemma_}\\t'\n",
    "                         f'{token.pos_}\\t{token.tag_}\\n')\n",
    "            f2.write('EOS\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_gen():\n",
    "    ''' 情報を一文ずつ取得 '''\n",
    "    with open('bocchan2.txt', encoding='utf-8') as f:\n",
    "        sequence = []\n",
    "        for line in f:\n",
    "            if line == 'EOS\\n':\n",
    "                yield sequence\n",
    "                sequence = []\n",
    "                continue\n",
    "            word_info = line.strip().split(\"\\t\")\n",
    "            if len(word_info) ==5:\n",
    "                pos = word_info[3].split(',')\n",
    "                pos2 = word_info[4].split(',')\n",
    "                sequence.append({'surface': word_info[1],\n",
    "                                 'base': word_info[2],\n",
    "                                 'pos': pos[0],\n",
    "                                 'pos2': pos2[0]\n",
    "                                })\n",
    "\n",
    "# for seq in sequence_gen():\n",
    "#     print(seq, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "赤シャツ 155件\n",
      "うらなり君 44件\n",
      "婆さん 29件\n",
      "坊っちゃん 14件\n",
      "控所 11件\n",
      "お婆さん 10件\n",
      "時間目 9件\n",
      "奴等 9件\n",
      "文学士 8件\n",
      "寄宿生 8件\n",
      "赤シャツさん 8件\n",
      "いか銀 7件\n",
      "うらなり先生 7件\n",
      "日目 6件\n",
      "精神的娯楽 6件\n",
      "お嬢さん 6件\n",
      "商買 5件\n",
      "お茶 5件\n",
      "団子屋 5件\n",
      "宿直部屋 5件\n",
      "お嫁 5件\n",
      "校長室 4件\n",
      "肝癪 4件\n",
      "赤手拭 4件\n",
      "尻持 4件\n",
      "難有 4件\n",
      "お目 4件\n",
      "爺さん 4件\n",
      "お母さん 4件\n",
      "日清談判 4件\n",
      "祝勝会 4件\n",
      "師範生 4件\n",
      "親譲り 3件\n",
      "日前 3件\n",
      "畳半 3件\n",
      "物理学校 3件\n",
      "面倒臭い 3件\n",
      "馬鹿気 3件\n",
      "楷子段 3件\n",
      "かみさん 3件\n",
      "辛防 3件\n",
      "お手際 3件\n",
      "前任者 3件\n",
      "蒟蒻版 3件\n",
      "徹頭徹尾 3件\n",
      "おっ母さん 3件\n",
      "別嬪さん 3件\n",
      "校長さん 3件\n",
      "師範学校 3件\n",
      "瓦斯燈 3件\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "pattern = re.compile('NN+')\n",
    "l = []\n",
    "\n",
    "for seq in sequence_gen():\n",
    "    encode_str = ''.join('N' if w['pos'] in ('NOUN') else '?' for w in seq)\n",
    "#     print(encode_str)\n",
    "    for m in pattern.finditer(encode_str):\n",
    "        l.append(''.join(w['surface'] for w in seq[m.start():m.end()]))\n",
    "\n",
    "# with open(\"bocchan_result.txt\", 'wt',encoding='utf-8') as f:\n",
    "#     for w, n in Counter(l).most_common(50):\n",
    "#         text = str(w) +\" \"+ str(n) +\"件\"+ '\\n'\n",
    "#         f.write(text)\n",
    "for w, n in Counter(l).most_common(50):\n",
    "    print(str(w) +\" \"+ str(n) +\"件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x2e3d6fda828>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "text = ' '.join(l)\n",
    "#日本語のパスらしいです\n",
    "fpath = \"C:/Windows/Fonts/YuGothM.ttc\"\n",
    "wordcloud = WordCloud(background_color=\"white\",#背景を白に\n",
    "                     font_path=fpath,width = 800,height=600).generate(text)\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "\n",
    "#pngで保存する\n",
    "wordcloud.to_file(\"bocchan.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
